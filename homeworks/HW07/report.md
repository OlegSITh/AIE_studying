# HW07 – Report

## 1. Datasets

Вы выбрали 3 датасета из 4:

### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (нужны строки, столбцы после удаления sample_id)
- Признаки: числовые (сколько столбцов)
- Пропуски: нет (или сколько)
- "Подлости": разные шкалы признаков, шумовые признаки

### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (строки, столбцы после sample_id)
- Признаки: числовые (сколько)
- Пропуски: нет (или сколько)
- "Подлости": нелинейная структура, выбросы, лишний шумовой признак

### 1.3 Dataset C

- Файл: `S07-hw-dataset-04.csv`
- Размер: (строки, столбцы после sample_id)
- Признаки: числовые (сколько) + категориальные (сколько, какие столбцы)
- Пропуски: есть в числовых признаках (сколько примерно в %)
- "Подлости": высокая размерность, категориальные признаки, пропуски

## 2. Protocol

Опишите ваш "честный" unsupervised-протокол:

- Препроцессинг:
  - Dataset 1, 2: только StandardScaler для числовых признаков
  - Dataset 3: SimpleImputer(strategy='median') для пропусков в числовых признаках + StandardScaler + OneHotEncoder для категориальных с handle_unknown='ignore'

- Поиск гиперпараметров:
  - KMeans: k от 2 до 20
  - DBSCAN: eps от 0.3 до 1.0, min_samples 5 или 10
  - Agglomerative: linkage методы ['ward', 'complete', 'average', 'single']
  - Выбор лучшего: по максимальному Silhouette Score, с учетом интерпретируемости

- Метрики: silhouette_score, davies_bouldin_score, calinski_harabasz_score
  - Для DBSCAN: метрики считаны на non-noise точках, доля шума указана отдельно

- Визуализация:
  - PCA(2D) для всех лучших алгоритмов
  - Графики: silhouette vs k для KMeans, silhouette vs eps для DBSCAN
  - t-SNE: (если делали) с perplexity=30, random_state=42

## 3. Models

### Dataset 1:
- KMeans: k от 2 до 20, random_state=42, n_init=10
- AgglomerativeClustering: k=оптимальное от KMeans, linkage=['ward', 'complete', 'average', 'single']

### Dataset 2:
- KMeans: k от 2 до 20, random_state=42, n_init=10
- DBSCAN: eps=[0.3, 0.5, 0.7, 1.0], min_samples=[5, 10]

### Dataset 3:
- KMeans: k от 2 до 20, random_state=42, n_init=10
- DBSCAN: eps=[1.5, 2.0, 2.5, 3.0], min_samples=5

## 4. Results

### 4.1 Dataset A

- Лучший метод и параметры: (KMeans/Agglomerative с какими параметрами)
- Метрики: silhouette=число, DB=число, CH=число
- Коротко: почему это решение выглядит разумным именно для этого датасета

### 4.2 Dataset B

- Лучший метод и параметры: (KMeans/DBSCAN с какими параметрами)
- Метрики: silhouette=число, DB=число, CH=число
- Если был DBSCAN: доля шума=число%, комментарий
- Коротко: почему это решение выглядит разумным именно для этого датасета

### 4.3 Dataset C

- Лучший метод и параметры: (KMeans/DBSCAN с какими параметрами)
- Метрики: silhouette=число, DB=число, CH=число
- Если был DBSCAN: доля шума=число%, комментарий
- Коротко: почему это решение выглядит разумным именно для этого датасета

## 5. Analysis

### 5.1 Сравнение алгоритмов

- Где KMeans "ломается" и почему? (Dataset 2 из-за нелинейной структуры)
- Где DBSCAN/иерархическая кластеризация выигрывают и почему? (Dataset 2 для DBSCAN)
- Что сильнее всего влияло на результат:
  - Масштабирование для Dataset 1
  - Выбросы/нелинейность для Dataset 2
  - Категориальные признаки и пропуски для Dataset 3

### 5.2 Устойчивость (для Dataset 1)

- Проверка: 5 запусков KMeans с разными random_state
- Результаты: средний ARI=число, стандартное отклонение=число
- Вывод: устойчиво/неустойчиво, потому что...

### 5.3 Интерпретация кластеров

- Анализ центроидов KMeans для Dataset 1 или профили признаков
- Выводы: например, "кластер 0 отличается высокими значениями признаков X и Y, кластер 1 - низкими значениями Z"

## 6. Conclusion

1. Масштабирование критически важно для KMeans и DBSCAN
2. KMeans работает только с шарообразными кластерами
3. DBSCAN хорош для выбросов и сложных форм, но требует подбора eps
4. Высокие размерности и категории требуют особой обработки
5. Метрики помогают, но нужно смотреть на визуализацию
6. Устойчивость важна - нужно проверять воспроизводимость
7. Предобработка часто важнее выбора алгоритма
8. Нет лучшего алгоритма на все случаи - нужен осознанный выбор